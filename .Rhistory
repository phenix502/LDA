source('~/code/jss/jss.R')
install.packages("tm")
remove_HTML_markup <- function(s){
tryCatch(
{
doc <- htmlTreeParse(paste("<!DOCTYPE html>", s),
asText =  TRUE, trim = FALSE)
xmlValue(xmlRoot(doc))
},error = function(s) s)
}
corpus <- Corpus(VectorSource(sapply(JSS_papers[,"description"],remove_HTML_markup)))
Sys.setlocale("LC_COLLATE", "C")
JSS_dtm <- DocumentTermMatrix(corpus)
library("tm")
library("XML")
remove_HTML_markup <- function(s){
tryCatch(
{
doc <- htmlTreeParse(paste("<!DOCTYPE html>", s),
asText =  TRUE, trim = FALSE)
xmlValue(xmlRoot(doc))
},error = function(s) s)
}
corpus <- Corpus(VectorSource(sapply(JSS_papers[,"description"],remove_HTML_markup)))
Sys.setlocale("LC_COLLATE", "C")
JSS_dtm <- DocumentTermMatrix(corpus)
dim(JSS_dtm)
vignette("topicmodels")
install.packages("topicmodels")
vignette(topicmodels)
vignette("topicmodels")
dtm <- DocumentTermMatrix(corpus,control = list(stemming = TRUE, stopwords = TRUE,
minWordLength = 3,
removeNumbers = TRUE))
dtm <- DocumentTermMatrix(corpus,control = list(stemming = TRUE, stopwords = TRUE,
minWordLength = 3,
removeNumbers = TRUE))
install.packages("Snowball")
dtm <- DocumentTermMatrix(corpus,control = list(stemming = TRUE, stopwords = TRUE,
minWordLength = 3,
removeNumbers = TRUE))
dtm <- removeSparseTerms(dtm, 0.99)
dim(dtm)
source('~/code/jss/jss.R')
install.packages("topicmodels")
library("tm")
library("topicmodels")
library("XML")
remove_HTML_markup <- function(s) {
doc <- htmlTreeParse(s, asText = TRUE, trim = FALSE)
iconv(xmlValue(xmlRoot(doc)), "", "UTF-8")
}
corpus <- Corpus(VectorSource(sapply(JSS_papers[, "description"],
remove_HTML_markup)))
dtm <- DocumentTermMatrix(corpus,control = list(stemming = TRUE, stopwords = TRUE, minWordLength = 3,
removeNumbers = TRUE))
dtm <- removeSparseTerms(dtm, 0.99)
dim(dtm)
corpus <- Corpus(VectorSource(sapply(JSS_papers[, "description"],
remove_HTML_markup)))
library("XML")
install.packages("XML")
remove_HTML_markup <- function(s) {
doc <- htmlTreeParse(s, asText = TRUE, trim = FALSE)
iconv(xmlValue(xmlRoot(doc)), "", "UTF-8")
}
orpus <- Corpus(VectorSource(sapply(JSS_papers[, "description"],
remove_HTML_markup)))
library("XML")
install.packages("XML")
library("XML")
library(XML)
library("tm")
library("topicmodels")
library("topicmodels")
install.packages("topicmodels")
install.packages("topicmodels")
install.packages("XML")
library("tm")
library("topicmodels")
library("XML")
install.packages("XML")
install.packages("XML")
library("XML")
remove_HTML_markup <- function(s) {
doc <- htmlTre(s, asText = TRUE, trim = FALSE)
iconv(xmlValue(xmlRoot(doc)), "", "UTF-8")
}
corpus <- Corpus(VectorSource(sapply(JSS_papers[, "description"],
remove_HTML_markup)))
remove_HTML_markup <- function(s) {
doc <- htmlTreeParse(s, asText = TRUE, trim = FALSE)
iconv(xmlValue(xmlRoot(doc)), "", "UTF-8")
}
corpus <- Corpus(VectorSource(sapply(JSS_papers[, "description"],
remove_HTML_markup)))
inspect(corpus)
dim(corpus)
dtm <- DocumentTermMatrix(corpus,control = list(stemming = TRUE, stopwords = TRUE, minWordLength = 3,
removeNumbers = TRUE))
dim(dtm)
dtm <- removeSparseTerms(dtm, 0.99)
dtm <- removeSparseTerms(dtm, 0.99)
dim(dtm)
jss_LDA <- LDA(dtm[1:250,],control = list(alpha = 0.1), k = 10)
jss_CTM <- CTM(dtm[1:250], k = 10)
post <- posterior(jss_LDA, newdata = dtm[-c(1:250),])
round(post$topics[1:5,],digits = 2)
get_terms(jss_LDA, 5)
class(jss_LDA)
typeof(jss_LDA)
JSS_papers
head(JSS_papers)
JSS_papers[, "description"]
class(JSS_papers[, "description"])
corpus
inspect(corpus)
dtm
class(dtm)
head(dtm)
inspect(dtm)
dtm <- removeSparseTerms(dtm, 0.99)
dim(dtm)
dtm
jss_LDA <- LDA(dtm[1:250,],control = list(alpha = 0.1), k = 10)
class(dtm)
dtm[1:5]
inspect(dtm[1:5])
class(jss_CTM)
class(topicmodels)
jss_LDA
inspect(jss_LDA)
summary(jss_LDA)
DocumentTermMatrix
jss_LDA
help(LDA_VEM)
help(topicmodles::LDA_VEM)
??LDA_VEM
>removeSparseTerms
?removeSparseTerms
class(jss_LDA)
?posterior
??posterior,TopicModel,ANY-meth
??posterior
??vectorsource
VectorSource
??VectorSource
cor.path <- "D:/Rcode/LDA/corpus"
cor <- Corpus(DirSource(directory = cor.path, encoding = "UTF-8"))
cor.path <- "corpus"
cor <- Corpus(DirSource(directory = cor.path, encoding = "UTF-8"))
library(tm)
library(topicmodels)
library(Rwordseg)
cor.path <- "corpus"
cor <- Corpus(DirSource(directory = cor.path, encoding = "UTF-8"))
cor
inspect(cor)
inspect(corpus)
inspect(cor)
cor.dtm <- DocumentTermMatrix(cor.cl,control = list(wordLengths = c(2, Inf),stopwords = mystopwords,removePunctuation= TRUE))
cor.cl <- tm_map(cor,stripWhitespace)
inspect(cor.cl)
cor.cl <- tm_map(cor.cl,removePunctuation)
inspect(cor.cl)
cor.cl <- tm_map(cor.cl,removeNumbers)
inspect(cor.cl)
cor.dtm <- DocumentTermMatrix(cor.cl,control = list(wordLengths = c(2, Inf),stopwords = mystopwords,removePunctuation= TRUE))
mystopwords <- readLines("mystopwords",encoding = "UTF-8")
mystopwords <- readLines("mystopwords.txt",encoding = "UTF-8")
stopwords <- read.table("~/code/jss/stopwords.txt", header=T, quote="\"")
View(stopwords)
mystopwords <- readLines("~/code/jss/mystopwords.txt",encoding = "UTF-8")
mystopwords <- readLines("stopwords.txt",encoding = "UTF-8")
mystopwords
mystopwords[12]
class(mystopwords)
cor.dtm <- DocumentTermMatrix(cor.cl,control = list(wordLengths = c(2, Inf),stopwords = mystopwords,removePunctuation= TRUE))
cor.dtm
inspect(cor.dtm)
dim(cor.dtm)
cor.dtm <- removeSparseTerms(cor.dtm, 0.99)
dim(cor.dtm)
inspect(cor.dtm)
cor.dtm([1:250])
cor.dtm([1:250],)
cor.dtm[1:250]
inspect(cor.dtm[1:250])
post <- posterior(result_LDA, newdata = dtm[-c(1:250),])
result_LDA <- LDA(cor.dtm[1:250,],control = list(alpha = 0.1), k = 10)
inspect(cor.dtm)
rowsum(dtm)
?rowsum
rowSums(dtm)
rowSums(cor.dtm)
class(cor.dtm)
cor.dtm['1.txt']
cor.dtm[1.txt]
cor.dtm['1.txt']
result_LDA <- LDA(cor.dtm[1:250,],control = list(alpha = 0.1), k = 10)
rowTotals <- apply(cor.dtm, 1, sum)
rowTotals
cor.dtm <- cor.dtm[rowTotals > 0]
dim(cor.dtm)
result_LDA <- LDA(cor.dtm[1:250,],control = list(alpha = 0.1), k = 10)
cor.dtm[-c(1:250)]
inspect(cor.dtm[-c(1:250)])
post <- posterior(result_LDA, newdata = cor.dtm[-c(1:250),])
round(post$topics[1:5,],digits = 2)
get_terms(jss_LDA, 5)
get_terms(result_LDA, 5)
result_LDA <- LDA(cor.dtm[1:250,],control = list(alpha = 0.1), k = 5)
post <- posterior(result_LDA, newdata = cor.dtm[-c(1:250),])
round(post$topics[1:5,],digits = 2)
get_terms(result_LDA, 5)
post <- posterior(result_LDA, newdata = cor.dtm[-c(1:250),])
cor.dtm[-c(1:250),]
result_LDA <- LDA(cor.dtm[1:250,],control = list(alpha = 0.1), k = 4)
jss_CTM <- CTM(cor.dtm[1:250], k = 10)
post <- posterior(result_LDA, newdata = cor.dtm[-c(1:250),])
round(post$topics[1:5,],digits = 2)
get_terms(result_LDA, 5)
result_LDA <- LDA(cor.dtm[1:250,],control = list(alpha = 0.1), k = 3)
# jss_CTM <- CTM(cor.dtm[1:250], k = 10)
post <- posterior(result_LDA, newdata = cor.dtm[-c(1:250),])
round(post$topics[1:5,],digits = 2)
get_terms(result_LDA, 5)
doc.name <- docname(400)
docname <- function(n){
doc.name <- paste(1:n,".txt",sep="")
}
doc.name <- docname(400)
doc.name
doc.name[3]
doc.name[4]
doc.name[400]
txt <- readLines("~/code/jss/clothe/1.txt", encoding = 'utf-8')
txt <- readLines("~/code/jss/clothe/2.txt", encoding = 'utf-8')
txt
txt[0]
txt[1]
txt[2]
txt[3]
txt[2]
txt[1]
unlist(txt)
lapply(txt,segmentCN)
words = lapply(txt,segmentCN)
words
words = unlist(lapply(txt,segmentCN))
words
?unlist
l.ex <- list(a = list(1:5, LETTERS[1:5]), b = "z", c =  NA)
l.ex
unlist(l.ex)
unlist(l.ex, recursive= FALSE)
unlist(l.ex, recursive= TRUE)
l = unlist(l.ex, recursive= TRUE)
l
l[1]
l[2]
l.ex[1]
l.ex[[1]]
l.ex[[2]]
sapply(txt,segmentCN)
txt = txt[txt!=" "]
txt
words = unlist(lapply(txt,segmentCN))
words
words[1]
words[2]
words[3]
cor_test <- Corpus(VectorSource(words))
cor_test
inspect(cor_test)
?paste
file.path <- paste("~/code/jss/clothe/", i, ".txt", sep = "")
file.path <- paste("~/code/jss/clothe/", 1, ".txt", sep = "")
file.path
words
for (i in 1:400){}
file.path <- paste("~/code/jss/clothe/", i, ".txt", sep = "")
txt <- readLines(file.path, encoding = 'utf-8')
## 去掉空白行
txt = txt[txt!=" "]
## 分词
words = unlist(lapply(txt,segmentCN))
file.save <- paste("corpus/", i, ".txt", sep = "")
write.table(words, file.save, fileEncoding = 'utf-8')
}
for (i in 1:400){
file.path <- paste("~/code/jss/clothe/", i, ".txt", sep = "")
txt <- readLines(file.path, encoding = 'utf-8')
## 去掉空白行
txt = txt[txt!=" "]
## 分词
words = unlist(lapply(txt,segmentCN))
file.save <- paste("corpus/", i, ".txt", sep = "")
write.table(words, file.save, fileEncoding = 'utf-8')
}
library(tm)
library(topicmodels)
#读取语料库的路径
cor.path <- "clean"
cor <- Corpus(DirSource(directory = cor.path, encoding = "UTF-8"))
cor.cl <- tm_map(cor,stripWhitespace)
cor.cl <- tm_map(cor.cl,removePunctuation)
cor.cl <- tm_map(cor.cl,removeNumbers)
## 加载停止词
mystopwords <- readLines("stopwords.txt",encoding = "UTF-8")
mystopwords
cor.dtm <- DocumentTermMatrix(cor.cl, control = list(wordLengths = c(2, Inf),stopwords = mystopwords,removePunctuation= TRUE))
cor.dtm
dim(cor.dtm)
inspect(cor.dtm)
cor.dtm <- removeSparseTerms(cor.dtm, 0.99)
rowTotals <- apply(cor.dtm, 1, sum)
cor.dtm <- cor.dtm[rowTotals > 0]
cor.dtm
result_LDA <- LDA(cor.dtm[1:250,],control = list(alpha = 0.1), k = 3)
post <- posterior(result_LDA, newdata = cor.dtm[-c(1:250),])
round(post$topics[1:5,],digits = 2)
get_terms(result_LDA, 5)
a<- -c(1:250)
a
post <- posterior(result_LDA, newdata = cor.dtm[-c(1:150),])
round(post$topics[1:5,],digits = 2)
get_terms(result_LDA, 5)
class(post)
post
get_terms(result_LDA, 5)
396+198
tongxing<-c(5,52,50,15,5,5,5,10)
sum(tongxing)
str(tongxing)
oil<-c(100,135)
sum(oil)
tax.1<-c(15,26,10,27,24,22,26,13)
tax.2<-c(20,12,48,24,17,21,23,19,21)
sum(tax.1)+sum(tax.2)
str(tax.1)
str(tax.2)
tax.3 <-c(23,17,10,10,43,10.8,12,23,30,23,23)
str(tax.)
str(tax.3)
sum(tax.3)
